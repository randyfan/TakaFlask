Submit a write-up that consists of a description of the web API that was built and how it can be used
# How to use:
We’ve included a README in our repo describing how to run our app:https://github.com/UCB-INFO-BACKEND-WEBARCH/group-project-taka
# Summary
Taka is an AI-powered speech-to-text tool that analyzes your speech and provides real-time and historical analytics. To activate Taka, simply go to our website on Google Chrome (https://sleepy-dawn-31618.herokuapp.com/), click on the microphone button, and Taka will begin listening to your voice, generating real-time analytics, such as flagging the overuse of filler words and repeated phrases, and providing suggestions (e.g., “You’re speaking too fast, slow down!”). Our API struggles picking up on “uhh” and “ahhs” as those are automatically filtered out by the ML model behind the scenes, but is able to capture other filler words.
# Endpoints
We use several endpoints for our app. A main page endpoint that renders HTML. 3 GET endpoints that correspond to different analyses: a speed endpoint: “/speed”, a filler words endpoint: “/filler”, a most frequent words endpoint: “/frequent”, and POST endpoint to update database with speech Endpoint “/save”.  
# External API:
For our external api, we used the HTML5 Speech Recognition API (https://shapeshed.com/html5-speech-recognition-api/)
This API allows JavaScript to have access to a browser’s audio stream and convert it to text. One of the main reasons we used this API is because it’s completely free [https://groups.google.com/a/chromium.org/g/chromium-html5/c/F99CPhKt0cM] (unlike Google’s Speech to Text API which requires trial or real credits). The API runs smoothly in the browser and is great at converting English speech into text based on our usage experience. 
Furthermore, since we wanted to implement a frontend as well, this API was a good choice for us as it could be implemented along with our HTML, JavaScript, and CSS code. 
The API receives input from the browser’s microphone and we store the transcribed speech in various data structures. Then, we use Ajax functions to send the transcribed speech to the endpoints (/speed, /filler, /frequent, and /save). The backend has several functions implemented in analyze_speech_funcs.py to further processes it and return the relevant results (e.g., # of filler words uttered).
# Database:
We used a mySQL database. We decided to store the data in a table with two columns. The primary key of the table is the id of the speech, and the cumulative utterances would store the entire speech as strings at that id. In this way, the user would be able to access the speech in the past and analyze them in any desired way. In the future, we could easily implement additional features that analyze historical speech data. Our database persists since it’s located in a separate docker container on initialization and stays alive if the server container gets shut down.

# Extra Credit:
We implemented a full stack app, so it includes a frontend. We had very limited experience with Javascript and CSS, so we’re quite proud of our final result. We learned how to use new libraries, such as Chart.js.
We also reimplemented the whole app in Go language (so technically building another API :-) ) without changing the Javascript code to test the separation between backend and frontend. It worked out beautifully and we actually deployed the Go lang version of Taka on Heroku: https://sleepy-dawn-31618.herokuapp.com/ . We also had zero prior experience with the Go language and Heroku, so there was a learning curve and familiarizing ourselves with the various Go libraries and deploying to Heroku. Fortunately, the concepts we learned in class helped us significantly because we found the Go version of our app to be conceptually identical to the Python Flask version. It was just a matter of changing the syntax to match the Go language. 
Randy also recently implemented a completely different type of API using websockets that plots real-time stock price data via websocket channels storing data that is transferred to a frontend. Since it’s different from the usage of REST API calls, we did not submit it but code is available upon request.
